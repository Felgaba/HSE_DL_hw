{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9bPPX09SQp"
      },
      "source": [
        "# В конце этого семинара - домашка!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_hC1iK_9SQu"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKqXqTC_9SQu"
      },
      "source": [
        "## __Tensors__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXzJO6mw9SQv"
      },
      "source": [
        "Одно из основных понятий в PyTorch -- это __Tenosor__. \n",
        "\n",
        "https://pytorch.org/docs/master/tensors.html\n",
        "\n",
        "__Tensor__ -- это такой же массив, как и в __numpy.array__, размерность и тип данных которого мы можем задать. Tensor в отличие от numpy.array может вычисляться на __GPU__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cR3cORW9SQv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_7O_-qC9SQw"
      },
      "outputs": [],
      "source": [
        "N = 100\n",
        "D_in = 50\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "x = np.random.randn(N, D_in)\n",
        "x_torch = torch.randn(N, D_in, device=device, dtype=dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvhesKe19SQx",
        "outputId": "37a8791f-3dc2-4d60-b746-b5e9a587003a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.9063605 ,  1.28727244,  1.04396854, ...,  0.61918339,\n",
              "        -0.77514123, -1.05039963],\n",
              "       [ 0.80203171, -0.33922492,  0.82562547, ...,  0.28729472,\n",
              "        -0.32674109, -0.78163499],\n",
              "       [-0.80135646, -0.51789672, -0.5729547 , ...,  0.68213647,\n",
              "        -0.24556942, -1.45290854],\n",
              "       ...,\n",
              "       [-0.85989426, -0.69044729, -0.78280206, ..., -1.48344825,\n",
              "         0.54556039, -1.62679144],\n",
              "       [-1.11647949,  1.11870765, -1.06416856, ...,  0.06115636,\n",
              "        -0.057212  ,  0.03762534],\n",
              "       [ 0.03395354, -0.21853534,  1.20785192, ...,  0.74579299,\n",
              "         0.30592849, -0.33713723]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqw-uK-o9SQy",
        "outputId": "1f35f2ce-df35-446c-b84f-9d488caea019"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7963,  0.2343,  0.7029,  ..., -0.0640, -0.3748, -0.7350],\n",
              "        [-1.8590, -0.0258, -1.6153,  ..., -1.0861, -0.1384, -0.8094],\n",
              "        [-0.8748, -0.7045,  0.3725,  ...,  0.6364, -0.1557,  0.9131],\n",
              "        ...,\n",
              "        [-0.5822,  0.8865, -0.2118,  ...,  1.4809, -0.3481,  0.1570],\n",
              "        [ 1.2604,  0.7188,  0.3066,  ..., -1.7161,  1.6516, -1.6850],\n",
              "        [-0.4700, -0.2816, -0.4469,  ..., -0.4257,  1.9322,  0.4812]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "x_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46jo1T1V9SQz",
        "outputId": "17e658f1-5216-4027-89e1-c4c31e9a946e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_torch = torch.Tensor(np.ones((N, D_in)))\n",
        "x_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFsmWbEM9SQ0",
        "outputId": "0ed8c8da-d178-43bd-dacd-1419cd0de70d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x_torch = torch.FloatTensor([1, 2, 3])\n",
        "x_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SetQ_qae9SQ0"
      },
      "outputs": [],
      "source": [
        "x1 = torch.IntTensor([1, 2, 3])\n",
        "x2 = torch.FloatTensor([3, 4, 5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI6Bp-V19SQ1",
        "outputId": "44709d03-09ff-440b-9c52-fc522c43772d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3], dtype=torch.int32)\n",
            "tensor([3., 4., 5.])\n"
          ]
        }
      ],
      "source": [
        "print(x1)\n",
        "print(x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eyvHPY99SQ1"
      },
      "source": [
        "В PyTorch можно найти много операций, которые похожи на то, что есть в numpy :\n",
        "```\n",
        "- torch.add (np.add) -> сложение тензоров (поэлементное)\n",
        "- torch.sub (np.subtract) -> вычитание (поэлементное)\n",
        "- torch.mul (np.multiply) -> умнажение скаляров / матриц (поэлементное)\n",
        "- torch.mm (np.matmul) -> перемножение матриц\n",
        "- torch.ones (np.ones) -> создание тензора из единиц\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b69D7WM09SQ4"
      },
      "outputs": [],
      "source": [
        "# Давайте попробуем вышепересчисленные операции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IBh10619SQ5"
      },
      "outputs": [],
      "source": [
        "x1 = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhbxUhjZ9SQ5"
      },
      "outputs": [],
      "source": [
        "x2 = torch.FloatTensor([[7, 8], [9, 1], [2, 3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01A1DKQm9SQ6",
        "outputId": "83ba98c3-ee3e-422c-d6c3-c20b3a060218"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[31., 19.],\n",
              "        [85., 55.]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "out = torch.mm(x1, x2)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fElEI3vJ9SQ6"
      },
      "source": [
        "```\n",
        "- torch.view (np.reshape) -> изменения порядка элементов в тензоре, не путать с транспонированием.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwCNZBay9SQ7"
      },
      "source": [
        "## Dynamic Computational Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS_eDX9s9SQ7"
      },
      "source": [
        "После того, как были реализованы архитектура модели и весь процес обучения и валидация сети, при запуске кода в PyTorch происходят следующие этапы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElP5dAse9SQ7"
      },
      "source": [
        "1. Строится вычислительный граф (направленный ациклический граф), где каждое ребро, ведущее к дргуому узлу, -- это тензор, а узел - это выполнение операции над данным тензором."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr359oqb9SQ8"
      },
      "source": [
        "<img src=\"./images/Graph.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID03QSfz9SQ8"
      },
      "source": [
        "Реализуем двухслойную сеть для задачи регрессии. И граф для такой архитектуры бдует выглядить следующим образом:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQwG4b819SQ8"
      },
      "source": [
        "<img src=\"./images/RegGraph.png\" alt=\"Drawing\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3aKkD0x9SQ8"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "input_size = 3\n",
        "hidden_size = 2\n",
        "output_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "gTriUHZ39SQ9",
        "outputId": "0ffa58d0-9edf-4296-9e08-78fa2070ed78"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e85fdb912248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad_w2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'grad_w1' is not defined"
          ]
        }
      ],
      "source": [
        "# Create random input and output data\n",
        "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
        "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    #TODO\n",
        "    \n",
        "    \n",
        "    # Compute and print loss\n",
        "    \n",
        "    \n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "    if t % 100 == 99:\n",
        "        print('Loss on iteration {} = {}'.format(t, loss))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-ECcY09SQ9"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix40kYZW9SQ-"
      },
      "source": [
        "## Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1yM-m7U9SQ-"
      },
      "source": [
        "2. Еще одно фундаментальное понятие и важный элемент при построении графа -- это __Autograd__ -- автоматическое дифференцирование.\n",
        "\n",
        "Для того чтобы с помощью стохастического градиентного спуска обновить обучаемые параметры сети, нужно посчитать градиенты. И как известно, обновление весов, которые учавтсвуют в нескольких операциях, происходит по `правилу дифференцирования сложной функции` (цепное правило или __chain rule__)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puy8JYGa9SQ-"
      },
      "source": [
        "<img src=\"./images/RegChainRule.png\" alt=\"Drawing\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KXSN6lI9SQ_"
      },
      "source": [
        "То есть (1) вычислительный граф позволяет определить последовательность операций, а (2) автоматическое дифференцирование посчитать нужные градиенты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijiWbvXH9SQ_"
      },
      "source": [
        "Если бы `Autograd` не было, то тогда backprop надо было бы реализовывать самим, и как это бы выглядело?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipiDfk5l9SQ_"
      },
      "source": [
        "Рассмотрим на примере, как посчиать градиенты для весов из входного слоя, где входной вектора `X` состоит из 3-х компонент. А входной слой вторую размерность имеет равной 2. \n",
        "\n",
        "После чего это идет в `ReLU`, но для простоты опустим на время ее, и посмотрим как дальше это идет по сети.\n",
        "\n",
        "Ниже написано, как это все вычисляется и приводит нас к значению целевой функции для одного наблюдения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBbJAPF9SQ_"
      },
      "source": [
        "<img src=\"./images/1.png\" alt=\"1\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIw_NEP39SRA"
      },
      "source": [
        "Тогда, чтобы посчитать градиент по первому элементу из обучаемой матрицы на первом слое, необходимо взять производоную у сложной функции. А этот как раз делается по `chain rule`: сначала берем у внешней, потом спускаемся на уровень ниже, и так пока не додйдем до то функции, после которой эта перменная уже нигде не участвует:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsjq24sF9SRA"
      },
      "source": [
        "<img src=\"./images/2.png\" alt=\"2\" style=\"width: 400px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8VzSOl99SRA"
      },
      "source": [
        "Перепишем это все в матричном виде, то есть сделаем аналог вида матрицы весов из первого слоя, но там уже будут её градиенты, котоыре будут нужны чтобы как раз обновить эти веса:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0Jb0M8Y9SRA"
      },
      "source": [
        "<img src=\"./images/3.jpg\" alt=\"3\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sdHZOKV9SRA"
      },
      "source": [
        "Как видно, здесь можно вектор X вынести, то есть разделить на две матрицы:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zqoKn09SRA"
      },
      "source": [
        "<img src=\"./images/4.jpg\" alt=\"4\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siT6BljP9SRB"
      },
      "source": [
        "То есть уже видно, что будем траспонировать входной вектор(матрицу). Но надо понимать, что в реальности у нас не одно наблюдение в батче, а несколько, тогда запись немного изменит свой вид:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9nffTj09SRB"
      },
      "source": [
        "<img src=\"./images/5.jpg\" alt=\"5\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DRyFSGS9SRB"
      },
      "source": [
        "Теперь мы видим, как на самом деле вычисляется вот те самые частные производные для вектора X, то есть видно, как математически это можно записать, а именно:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U44gdtsV9SRB"
      },
      "source": [
        "<img src=\"./images/6.jpg\" alt=\"6\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9_elHoX9SRB"
      },
      "source": [
        "<img src=\"./images/7.jpg\" alt=\"7\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoc95IcQ9SRB"
      },
      "source": [
        "Уже можно реализовать. Понятно, что транспонируется, что нет, и что на что умножается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbYaBe609SRD"
      },
      "source": [
        "Но помним про ReLU. Для простоты опустили, но теперь её учесть будет легче. \n",
        "\n",
        "Так как после первого слоя идет ReLU, а значит, занулились те выходы первого слоя, которые были __меньше__ нуля. Получается, что во второй слой не все дошло, тогда нужно обнулить, что занулил ReLU. \n",
        "\n",
        "Что занулил ReLU, мы можем выяснить при `forward pass`, а где именно поставить нули, то надо уже смотреть относительно `backward propagation`, на том выходе, где последний раз участвовал выход после ReLU, то есть:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM6oy0fo9SRD"
      },
      "source": [
        "<img src=\"./images/8.jpg\" alt=\"8\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-xrIPFc9SRD"
      },
      "source": [
        "Благодаря `Autograd` реализацию `chain rule` можно избежать, так как для более сложных нейронных сетей вручную такое реализовать сложно, при этом сделать это эффективным."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoIAWUqY9SRE"
      },
      "source": [
        "Для того чтобы PyTorch понял, за какими переменными надо \"следить\", то есть указать, что именно \"эти\" переменные являются обучаемыми, необходимо при создании тензора в качестве аттрибута указать __requires_grad=True__:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nld0dXc9SRE"
      },
      "outputs": [],
      "source": [
        "w1 = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWF9I8Cv9SRE"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    # Теперь подсчет градиентов для весов происходит при вызове backward\n",
        "    loss.backward()\n",
        "   \n",
        "    # Обновляем значение весов, но укзаываем, чтобы PyTorch не считал эту операцию, \n",
        "    # которая бы учавствовала бы при подсчете градиентов в chain rule\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "        \n",
        "        # Теперь обнуляем значение градиентов, чтобы на следующем шаге \n",
        "        # они не учитывались при подсчете новых градиентов,\n",
        "        # иначе произойдет суммирвоание старых и новых градиентов\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbeorp4a9SRE"
      },
      "source": [
        "Осталось еще не вручную обновлять веса, а использовать адаптивные методы градинетного спсука. Для этого нужно использовать модуль __optim__. А помимо оптимайзера, еще можно использовать готовые целевые функции из модлуя __nn__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTsOAlL59SRF"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.Adam([w1, w2], lr=learning_rate)\n",
        "\n",
        "for t in range(500):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "    \n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    loss.backward()\n",
        "   \n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aw1SN9-9SRF"
      },
      "source": [
        "После того, как мы сделали backward, в этот момент посчитались градиенты и граф уничтожился, то есть стёрлись все пути, которые связывали тензоры между собой. Это значит, что еще раз backward сделать не поулчится, будет ошибка. Но если вдруг нужно считать градиенты еще раз, то нужно при вызове backward задать `retain_graph=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQNexK5Z9SRF"
      },
      "source": [
        "Еще важный аттрибут, который есть у Tensor -- это `grad_fn`. В этом аттрибуте указывается та функция, посредством которой был создан этот тензор. Так PyTorch понимает, как именно считать по нему градиент."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2PT6XYn9SRG"
      },
      "outputs": [],
      "source": [
        "y_pred.grad_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7G5u-ww9SRG"
      },
      "source": [
        "Также можно контролировать, должны ли градиенты течь или нет."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwXhulaX9SRG"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.], requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    with torch.enable_grad():\n",
        "        y = x * 2\n",
        "y.requires_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUI3cLtD9SRG"
      },
      "source": [
        "## Почему Backprop надо понимать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PYkJByc9SRH"
      },
      "source": [
        "1. Backprop позволяет понимать, как те или иные операции, сложные конструкции в сети влияют на обнолвение весов.\n",
        "Почему лучше сделать конкатенацию тензоров, а не поэлементное сложение. Для этого нужно посмотреть на backprop, как будут обновляться веса.\n",
        "\n",
        "2. Даже на таком маленьком пример двуслойной MLP можно уже увидеть, когда `ReLU`, как функция активация, не очень хорошо применять. Если разреженные данные, то получить на выходе много нулей вероятнее, чем при использовании `LeakyReLU`, то есть градиенты будут нулевыми и веса никак не будут обновляться => сеть не обучается!\n",
        "\n",
        "3. В архитектуре могут встречаться недифференцируемые операции, и первое - это нужно понять, потому что при обучении сети это может быть не сразу заметно, просто качество модели будет плохое, и точность хорошую не поулчится достичь.\n",
        "\n",
        "Например, в одной из статей было предложено в качестве механизма внимания применить распределение Бернулли, которое умножается на выход промежуточного слоя сети. И эта опреация недифференцируема, нужно реализовывать backprop самим, тем самым обеспечить корректное протекание градиентов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAjhgQfN9SRH"
      },
      "source": [
        "<img src=\"./images/Bernoulli.png\" alt=\"8\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPLXhf2C9SRH"
      },
      "source": [
        "Так же любая статья, которая предлагет новую целевую функцию для той или иной задачи, там всегда будут представлены градиенты, чтобы было понимание, как это влияет на обновление весов. Не просто так !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Shl90k09SRH"
      },
      "source": [
        "<img src=\"./images/BernoulliBackProp.png\" alt=\"8\" style=\"width: 600px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDsiF5fG9SRJ"
      },
      "source": [
        "# Домашние задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMH__8G49SRJ"
      },
      "source": [
        "1. Добавить Bias и посчитать для них градиенты.\n",
        "2. Сравнить градинеты с тем, как считает PyTorch AutoGrad."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "3p2RLP5JV-Pb"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы сравнивать одинаковые исходные данные, зададим `x, y, w1, w1` и тд. сразу с `requires_grad=True`, иначе у нас будет 2 инициализации с разными значениями."
      ],
      "metadata": {
        "id": "PqJ-ESV6kxb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "input_size = 3\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(batch_size, input_size, device=device, dtype=dtype)\n",
        "y = torch.randn(batch_size, output_size, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1_start = torch.randn(input_size, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
        "w2_start = torch.randn(hidden_size, output_size, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "# Randomly initialize bias\n",
        "b1_start = torch.randn(1, hidden_size, device=device, dtype=dtype, requires_grad=True)\n",
        "b2_start = torch.randn(1, output_size, device=device, dtype=dtype, requires_grad=True)"
      ],
      "metadata": {
        "id": "CAXG9VrWSJdL"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По какой-то причине \"градиент вручную\" требует клонировать исходые данные, а AutoGrad просто знак =. Клонирование уже не подойдёт. Честно говоря, не понял, почему так."
      ],
      "metadata": {
        "id": "XMq6-IZGWuJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a clone so the original matrix is not affected!!!\n",
        "w1 = w1_start.clone()\n",
        "w2 = w2_start.clone()\n",
        "b1 = b1_start.clone()\n",
        "b2 = b2_start.clone()\n",
        "###"
      ],
      "metadata": {
        "id": "TGau4soZTLDU"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$h_1 = X \\cdot w_1 + b_1$  \n",
        "$h_{Relu} = max(0, h_1)$  \n",
        "$out = h_{Relu} \\cdot w_2 +b_2$  \n",
        "$loss = \\sum(out-y)^2$"
      ],
      "metadata": {
        "id": "loDTGAzpLDvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\dfrac{\\partial loss}{\\partial out} = 2(out-y)$  \n",
        "$grad(w_2) = h_{Relu}^T \\cdot \\dfrac{\\partial loss}{\\partial out}$  \n",
        "$grad(h_{Relu}) = \\dfrac{\\partial loss}{\\partial out} \\cdot w_2^T$  \n",
        "$grad(w_1) = h_{Relu}^T \\cdot \\dfrac{\\partial loss}{\\partial out}$  \n",
        "$grad(b_2) = \\dfrac{\\partial loss}{\\partial out}$  \n",
        "$grad(b_1) = \\dfrac{\\partial loss}{\\partial out} \\cdot w_2^T$"
      ],
      "metadata": {
        "id": "eSpWGKXdLGfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "for t in range(1001):\n",
        "    # Forward pass: compute predicted y\n",
        "    #TODO\n",
        "    h_1 = x.mm(w1) + b1\n",
        "    h_relu = h_1.clamp(min=0)\n",
        "    out = h_relu.mm(w2) + b2\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = (out - y).pow(2).sum().item()\n",
        "    \n",
        "    # Backward pass: \n",
        "    dloss_dout = 2 * (out - y)\n",
        "    \n",
        "    grad_w2 = h_relu.t().mm(dloss_dout) \n",
        "    \n",
        "    grad_h_relu = dloss_dout.mm(w2.t())\n",
        "    grad_h_relu[h_1 < 0] = 0\n",
        "    \n",
        "    grad_w1 = x.t().mm(grad_h_relu)\n",
        "    grad_w2 = h_relu.t().mm(dloss_dout) \n",
        "\n",
        "    grad_b2 = dloss_dout\n",
        "\n",
        "    grad_h_1 = dloss_dout.mm(w2.t())\n",
        "    grad_h_1[h_1 < 0] = 0\n",
        "    grad_b1 = grad_h_1\n",
        "    \n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "    b1 -= learning_rate * grad_b1.sum()\n",
        "    b2 -= learning_rate * grad_b2.sum()\n",
        "    if t % 100 == 0:\n",
        "        print('Loss on iteration {} = {}'.format(t, loss))"
      ],
      "metadata": {
        "id": "CmT-UGOiEKQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ed0062-0298-4802-eabb-672649ba913f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on iteration 0 = 117.30741119384766\n",
            "Loss on iteration 100 = 64.10514831542969\n",
            "Loss on iteration 200 = 55.514095306396484\n",
            "Loss on iteration 300 = 52.365882873535156\n",
            "Loss on iteration 400 = 51.388675689697266\n",
            "Loss on iteration 500 = 50.68135452270508\n",
            "Loss on iteration 600 = 50.15719223022461\n",
            "Loss on iteration 700 = 49.63975524902344\n",
            "Loss on iteration 800 = 49.18128967285156\n",
            "Loss on iteration 900 = 48.78951644897461\n",
            "Loss on iteration 1000 = 48.49049758911133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moTYB5yr9SRJ"
      },
      "source": [
        "### PyTorch AutoGrad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### reassign the initial random matrixes, otherwise it will use already modified values!!\n",
        "w1 = w1_start\n",
        "w2 = w2_start\n",
        "b1 = b1_start\n",
        "b2 = b2_start"
      ],
      "metadata": {
        "id": "YLxNyWEATeio"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "4zi2HQAh9SRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef230def-db62-4527-fd63-452659eb30c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 117.30741119384766\n",
            "100 64.7081298828125\n",
            "200 55.12907028198242\n",
            "300 51.97211456298828\n",
            "400 50.86362075805664\n",
            "500 50.217124938964844\n",
            "600 49.89947509765625\n",
            "700 49.518924713134766\n",
            "800 49.07821273803711\n",
            "900 48.718868255615234\n",
            "1000 48.528045654296875\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 1e-4\n",
        "for t in range(1001):\n",
        "    y_pred = (x.mm(w1) + b1).clamp(min=0).mm(w2) + b2\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 0:\n",
        "        print(t, loss.item())\n",
        "    \n",
        "    # Теперь подсчет градиентов для весов происходит при вызове backward\n",
        "    loss.backward()\n",
        "   \n",
        "    # Обновляем значение весов, но укзаываем, чтобы PyTorch не считал эту операцию, \n",
        "    # которая бы учавствовала бы при подсчете градиентов в chain rule\n",
        "    with torch.no_grad():          \n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "        b1 -= learning_rate * b1.grad\n",
        "        b2 -= learning_rate * b2.grad\n",
        "        \n",
        "        # Теперь обнуляем значение градиентов, чтобы на следующем шаге \n",
        "        # они не учитывались при подсчете новых градиентов,\n",
        "        # иначе произойдет суммирвоание старых и новых градиентов\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()\n",
        "        b1.grad.zero_()\n",
        "        b2.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Хотя результаты похожи, в зависимости от номера попытки они то увеличиваются, то снижаются. Были случаи, когда autograd давал большую ошибку, чем \"ручной градиент\". \n",
        "Собственные мысли на этот счёт: особенности арифметических операций Python; непрямолинейный подход в Autograd, из-за чего может иметь большую ошибку. Или..?"
      ],
      "metadata": {
        "id": "Jz9wmZtqZGFw"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "PyTorch.Graph.AutoGrad.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}